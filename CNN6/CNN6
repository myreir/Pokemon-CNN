import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

class EarlyStopping:
    """Early stops the training if validation loss doesn't improve after a given patience."""
    def __init__(self, patience=5, delta=0):
        self.patience = patience
        self.delta = delta
        self.best_loss = None
        self.early_stop = False
        self.counter = 0

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies):
    """Plots loss and accuracy graphs."""
    epochs = range(1, len(train_losses) + 1)
    
    plt.figure(figsize=(12, 6))
    # Loss plot
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, label="Train Loss")
    plt.plot(epochs, val_losses, label="Validation Loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Loss vs Epochs")
    plt.legend()
    
    # Accuracy plot
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_accuracies, label="Train Accuracy")
    plt.plot(epochs, val_accuracies, label="Validation Accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.title("Accuracy vs Epochs")
    plt.legend()

    plt.tight_layout()
    plt.show()

def compute_classwise_accuracy(model, dataloader, device, class_names):
    """Calculates per-class accuracy."""
    model.eval()
    class_correct = [0] * len(class_names)
    class_total = [0] * len(class_names)
    
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            
            for i in range(len(labels)):
                label = labels[i]
                class_correct[label] += (preds[i] == label).item()
                class_total[label] += 1
    
    class_accuracies = [correct / total if total > 0 else 0 for correct, total in zip(class_correct, class_total)]
    
    # Plotting per-class accuracy
    plt.figure(figsize=(10, 6))
    plt.bar(class_names, class_accuracies, color='blue')
    plt.xlabel("Classes")
    plt.ylabel("Accuracy")
    plt.title("Per-Class Accuracy")
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()

    return class_accuracies

def train_model(batch_size=32, learning_rate=0.000838635, epochs=100, patience=3):
    #check if CUDA is available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    #data transformations
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    #load the datasets
    train_data_dir = 'C:/Users/bwalk/OneDrive/Documents/Fall 2024/CS 471 - Intro To Artificial Intelligence/Final Assignment/preprocessed_data'
    val_data_dir = 'C:/Users/bwalk/OneDrive/Documents/Fall 2024/CS 471 - Intro To Artificial Intelligence/Final Assignment/TestData'

    train_dataset = datasets.ImageFolder(root=train_data_dir, transform=data_transforms['train'])
    val_dataset = datasets.ImageFolder(root=val_data_dir, transform=data_transforms['val'])

    #data loaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)

    #load a pre-trained model and customize it
    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
    for param in model.parameters():
        param.requires_grad = False
    num_ftrs = model.fc.in_features
    model.fc = nn.Sequential(
        nn.Linear(num_ftrs, 512),
        nn.ReLU(),
        nn.Dropout(0.5),
        nn.Linear(512, len(train_dataset.classes))  #adjust for number of classes
    )
    model = model.to(device)

    #define loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

    #early stopping
    early_stopping = EarlyStopping(patience=patience)

    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []

    #training loop
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            _, preds = torch.max(outputs, 1)
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

        epoch_loss = running_loss / len(train_loader.dataset)
        epoch_acc = running_corrects.double() / len(train_loader.dataset)
        train_losses.append(epoch_loss)
        train_accuracies.append(epoch_acc.item())
        print(f"Epoch {epoch + 1}/{epochs} - Train Loss: {epoch_loss:.4f} - Train Accuracy: {epoch_acc:.4f}")

        # Validation loop
        model.eval()
        val_running_loss = 0.0
        val_running_corrects = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, preds = torch.max(outputs, 1)
                val_running_loss += loss.item() * inputs.size(0)
                val_running_corrects += torch.sum(preds == labels.data)

        val_loss = val_running_loss / len(val_loader.dataset)
        val_acc = val_running_corrects.double() / len(val_loader.dataset)
        val_losses.append(val_loss)
        val_accuracies.append(val_acc.item())
        print(f"Epoch {epoch + 1}/{epochs} - Val Loss: {val_loss:.4f} - Val Accuracy: {val_acc:.4f}")

        scheduler.step()

        #early stopping check
        early_stopping(val_loss)
        if early_stopping.early_stop:
            print("Early stopping triggered")
            break

    # Plot metrics
    plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies)

    # Per-class accuracy for validation set
    compute_classwise_accuracy(model, val_loader, device, train_dataset.classes)

if __name__ == "__main__":
    train_model(batch_size=32, learning_rate=0.000838635, epochs=50, patience=3)
